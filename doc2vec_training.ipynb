{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Doc2vec Training\n",
    "\n",
    "Install packages to environment.\n",
    "\n",
    "TODO:\n",
    "Run Doc2Vec analysis on sentiment of documents\n",
    "Experiment with incorporating summarization\n",
    "Choose the best Doc2Vec model\n",
    "\n",
    "Testing changes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -U -q PyDrive\n",
    "!pip install gensim\n",
    "# Used by Gensim\n",
    "!pip install testfixtures\n",
    "!pip install scikit-learn\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following snippet to load article json files from Google Drive. Utility function save_to_drive can be used to save files to Google Drive. Useful for training models in Google Collab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "1dkWFcIoZoao",
    "outputId": "b528d84e-ab09-4ee3-d09c-fcf1e963dca1",
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "# from pydrive.auth import GoogleAuth\n",
    "# from pydrive.drive import GoogleDrive\n",
    "# from google.colab import auth\n",
    "# from oauth2client.client import GoogleCredentials\n",
    "# from google.colab import auth\n",
    "# import io\n",
    "# from googleapiclient.http import MediaIoBaseDownload\n",
    "# from googleapiclient.http import MediaFileUpload\n",
    "# \n",
    "# auth.authenticate_user()\n",
    "# from googleapiclient.discovery import build\n",
    "# drive_service = build('drive', 'v3')\n",
    "# \n",
    "# \n",
    "# \n",
    "# files_to_load = list()\n",
    "# \n",
    "# files_to_load.append({\n",
    "#     \"file_name\": \"all_articles.json\",\n",
    "#     \"id\": \"19ErkUdKHwJO46T3u_LnoUhU-Ol4Om90W\",\n",
    "#     \"is_binary\": 0\n",
    "# })\n",
    "# \n",
    "# \n",
    "# def download_from_drive(file_id):\n",
    "# \n",
    "#     request = drive_service.files().get_media(fileId=file_id)\n",
    "#     downloaded = io.BytesIO()\n",
    "#     downloader = MediaIoBaseDownload(downloaded, request)\n",
    "#     done = False\n",
    "#     while done is False:\n",
    "#         # _ is a placeholder for a progress object that we ignore.\n",
    "#         # (Our file is small, so we skip reporting progress.)\n",
    "#         _, done = downloader.next_chunk()\n",
    "# \n",
    "#     downloaded.seek(0)\n",
    "#     read = downloaded.read()\n",
    "#     return read\n",
    "# \n",
    "# \n",
    "# def save_to_drive(filename):\n",
    "# \n",
    "#     file_metadata = {\n",
    "#         'name': filename,\n",
    "#         'mimeType': 'text/plain'\n",
    "#     }\n",
    "#     media = MediaFileUpload(filename,\n",
    "#                             mimetype='text/plain',\n",
    "#                             resumable=True)\n",
    "#     created = drive_service.files().create(body=file_metadata,\n",
    "#                                            media_body=media,\n",
    "#                                            fields='id').execute()\n",
    "#     print('File ID: {}'.format(created.get('id')))\n",
    "# \n",
    "\n",
    "# # Download all the Google Drive files\n",
    "# for file in files_to_load:\n",
    "#     print(file)\n",
    "# \n",
    "#     # load document\n",
    "#     doc = download_from_drive(file[\"id\"])\n",
    "# \n",
    "#     text_file = str()\n",
    "#     if file[\"is_binary\"]:\n",
    "#         text_file = open(file[\"file_name\"], \"wb\")\n",
    "# \n",
    "#     else:\n",
    "#         doc = doc.decode(\"utf-8\")\n",
    "#         text_file = open(file[\"file_name\"], \"w\")\n",
    "# \n",
    "#     text_file.write(doc)\n",
    "#     text_file.close()\n",
    "# \n",
    "#     print(\"loaded: \" + file[\"file_name\"])\n",
    "\n",
    "\n",
    "print(\"Files loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GEC4EdY0hAde"
   },
   "source": [
    "## Finding optimal Doc2vec model for IMDB article sentiment prediction\n",
    "\n",
    "Download the [Large Movie Review Dataset](http://ai.stanford.edu/~amaas/data/sentiment/).\n",
    "\n",
    "Load dataset of articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from gensim.utils import to_unicode\n",
    "from nltk.tokenize import word_tokenize\n",
    "import collections\n",
    "import tarfile\n",
    "import re\n",
    "\n",
    "import random\n",
    "from random import shuffle\n",
    "random.seed(10)\n",
    "\n",
    "\n",
    "number_of_articles = 100000\n",
    "\n",
    "SentimentDocument = collections.namedtuple('SentimentDocument', 'words tags split sentiment')\n",
    "\n",
    "def create_sentiment_document(name, text, index):\n",
    "    \n",
    "    # Split the name of a movie review file into train/test, and +/- sentiment\n",
    "    _, split, sentiment_str, _ = name.split('/')\n",
    "    sentiment = {'pos': 1.0, 'neg': 0.0, 'unsup': None}[sentiment_str]\n",
    "\n",
    "    if sentiment is None:\n",
    "        split = 'extra'\n",
    "\n",
    "    tokens = word_tokenize(to_unicode(text))\n",
    "    return SentimentDocument(tokens, [index], split, sentiment)\n",
    "\n",
    "def extract_documents(imdb_file):\n",
    "    \n",
    "    index = 0\n",
    "\n",
    "    with tarfile.open(imdb_file, mode='r:gz') as tar:\n",
    "        for member in tar.getmembers():\n",
    "            if re.match(r'aclImdb/(train|test)/(pos|neg|unsup)/\\d+_\\d+.txt$', member.name):\n",
    "                member_bytes = tar.extractfile(member).read()\n",
    "                member_text = member_bytes.decode('utf-8', errors='replace')\n",
    "                assert member_text.count('\\n') == 0\n",
    "                yield create_sentiment_document(member.name, member_text, index)\n",
    "                index += 1\n",
    "                \n",
    "print(\"Loading documents ...\")             \n",
    "alldocs = list(extract_documents('aclImdb_v1.tar.gz'))\n",
    "shuffle(alldocs)\n",
    "alldocs = alldocs[:number_of_articles]\n",
    "\n",
    "print(f\"Total docs {len(alldocs)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Based on the [“Distributed Representations of Sentences and Documents”](http://cs.stanford.edu/~quocle/paragraph_vector.pdf) paper [Radim Hurek's](https://radimrehurek.com/gensim/auto_examples/howtos/run_doc2vec_imdb.html) reproduction of the experiment the `Doc2Vec(dbow,d100,n5,mc2,t8` model produces the lowest error rate in sentiment classification (10.3%).\n",
    "\n",
    "This model is a concatenation of the Distributed Bag of Words model model and the DM/mean model.\n",
    "\n",
    "To analyze whether further preprocessing of the text improves the quality of the embeddings we will train a model model for Text that went through a preprocessing pipeline containing:\n",
    "- Frequency based summarization, lematization, stopword removal, and contraction expansion\n",
    "- Lematization, stopword removal\n",
    "- Contraction expansion, stopword removal\n",
    "- Raw text\n",
    "\n",
    "To evaluate the quality of the embeddings, we will check the accuracy of the sentiment analysis model trained to predict the sentiment based on the document embedding.\n",
    "\n",
    "Define all of the preproccesing functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/milanarezina/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from string import punctuation\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "\n",
    "\n",
    "def substitute_contraction(word):\n",
    "    \"\"\"\n",
    "    Substitutes the contraction with expanded form\n",
    "    Substates non-ascii quotes\n",
    "    :param word:\n",
    "    :return:\n",
    "        The substituted contraction or the original token\n",
    "    \"\"\"\n",
    "\n",
    "    # Replace unicode commas\n",
    "    punctuation = {0x2018: 0x27, 0x2019: 0x27, 0x201C: 0x22, 0x201D: 0x22}\n",
    "    w = word.translate(punctuation)\n",
    "\n",
    "    contractions = get_contraction_dict()\n",
    "\n",
    "    if w in contractions.keys():\n",
    "        subbed = contractions[w]\n",
    "        return subbed\n",
    "    else:\n",
    "        return w\n",
    "    \n",
    "def get_contraction_dict():\n",
    "    return {\n",
    "        \"ain't\": \"is not\",\n",
    "        \"aren't\": \"are not\",\n",
    "        \"can't\": \"cannot\",\n",
    "        \"can't've\": \"cannot have\",\n",
    "        \"'cause\": \"because\",\n",
    "        \"could've\": \"could have\",\n",
    "        \"couldn't\": \"could not\",\n",
    "        \"couldn't've\": \"could not have\",\n",
    "        \"didn't\": \"did not\",\n",
    "        \"doesn't\": \"does not\",\n",
    "        \"don't\": \"do not\",\n",
    "        \"hadn't\": \"had not\",\n",
    "        \"hadn't've\": \"had not have\",\n",
    "        \"hasn't\": \"has not\",\n",
    "        \"haven't\": \"have not\",\n",
    "        \"he'd\": \"he would\",\n",
    "        \"he'd've\": \"he would have\",\n",
    "        \"he'll\": \"he will\",\n",
    "        \"he'll've\": \"he will have\",\n",
    "        \"he's\": \"he has\",\n",
    "        \"how'd\": \"how did\",\n",
    "        \"how'd'y\": \"how do you\",\n",
    "        \"how'll\": \"how will\",\n",
    "        \"how's\": \"how is\",\n",
    "        \"I'd\": \"I would\",\n",
    "        \"I'd've\": \"I would have\",\n",
    "        \"I'll\": \"I will\",\n",
    "        \"I'll've\": \"I shall have\",\n",
    "        \"I'm\": \"I am\",\n",
    "        \"I've\": \"I have\",\n",
    "        \"isn't\": \"is not\",\n",
    "        \"it'd\": \"it would\",\n",
    "        \"it'd've\": \"it would have\",\n",
    "        \"it'll\": \"it will\",\n",
    "        \"it'll've\": \"it shall have\",\n",
    "        \"it's\": \"it is\",\n",
    "        \"let's\": \"let us\",\n",
    "        \"ma'am\": \"madam\",\n",
    "        \"mayn't\": \"may not\",\n",
    "        \"might've\": \"might have\",\n",
    "        \"mightn't\": \"might not\",\n",
    "        \"mightn't've\": \"might not have\",\n",
    "        \"must've\": \"must have\",\n",
    "        \"mustn't\": \"must not\",\n",
    "        \"mustn't've\": \"must not have\",\n",
    "        \"needn't\": \"need not\",\n",
    "        \"needn't've\": \"need not have\",\n",
    "        \"o'clock\": \"of the clock\",\n",
    "        \"oughtn't\": \"ought not\",\n",
    "        \"oughtn't've\": \"ought not have\",\n",
    "        \"shan't\": \"shall not\",\n",
    "        \"sha'n't\": \"shall not\",\n",
    "        \"shan't've\": \"shall not have\",\n",
    "        \"she'd\": \"she would\",\n",
    "        \"she'd've\": \"she would have\",\n",
    "        \"she'll\": \"she will\",\n",
    "        \"she'll've\": \"she will have\",\n",
    "        \"she's\": \"she is\",\n",
    "        \"should've\": \"should have\",\n",
    "        \"shouldn't\": \"should not\",\n",
    "        \"shouldn't've\": \"should not have\",\n",
    "        \"so've\": \"so have\",\n",
    "        \"so's\": \"so is\",\n",
    "        \"that'd\": \"that had\",\n",
    "        \"that'd've\": \"that would have\",\n",
    "        \"that's\": \"that is\",\n",
    "        \"there'd\": \"there would\",\n",
    "        \"there'd've\": \"there would have\",\n",
    "        \"there's\": \"there is\",\n",
    "        \"they'd\": \"they would\",\n",
    "        \"they'd've\": \"they would have\",\n",
    "        \"they'll\": \"they will\",\n",
    "        \"they'll've\": \"they will have\",\n",
    "        \"they're\": \"they are\",\n",
    "        \"they've\": \"they have\",\n",
    "        \"to've\": \"to have\",\n",
    "        \"wasn't\": \"was not\",\n",
    "        \"we'd\": \"we would\",\n",
    "        \"we'd've\": \"we would have\",\n",
    "        \"we'll\": \"we will\",\n",
    "        \"we'll've\": \"we will have\",\n",
    "        \"we're\": \"we are\",\n",
    "        \"we've\": \"we have\",\n",
    "        \"weren't\": \"were not\",\n",
    "        \"what'll\": \"what will\",\n",
    "        \"what'll've\": \"what will have\",\n",
    "        \"what're\": \"what are\",\n",
    "        \"what's\": \"what is\",\n",
    "        \"what've\": \"what have\",\n",
    "        \"when's\": \"when is\",\n",
    "        \"when've\": \"when have\",\n",
    "        \"where'd\": \"where did\",\n",
    "        \"where's\": \"where is\",\n",
    "        \"where've\": \"where have\",\n",
    "        \"who'll\": \"who will\",\n",
    "        \"who'll've\": \"who will have\",\n",
    "        \"who's\": \"who is\",\n",
    "        \"who've\": \"who have\",\n",
    "        \"why's\": \"why is\",\n",
    "        \"why've\": \"why have\",\n",
    "        \"will've\": \"will have\",\n",
    "        \"won't\": \"will not\",\n",
    "        \"won't've\": \"will not have\",\n",
    "        \"would've\": \"would have\",\n",
    "        \"wouldn't\": \"would not\",\n",
    "        \"wouldn't've\": \"would not have\",\n",
    "        \"y'all\": \"you all\",\n",
    "        \"y'all'd\": \"you all would\",\n",
    "        \"y'all'd've\": \"you all would have\",\n",
    "        \"y'all're\": \"you all are\",\n",
    "        \"y'all've\": \"you all have\",\n",
    "        \"you'd\": \"you would\",\n",
    "        \"you'd've\": \"you would have\",\n",
    "        \"you'll\": \"you will\",\n",
    "        \"you'll've\": \"you will have\",\n",
    "        \"you're\": \"you are\",\n",
    "        \"you've\": \"you have\"\n",
    "    }\n",
    "\n",
    "\n",
    "class LanguageProcessor:\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        All of the natural processing functionality\n",
    "        \"\"\"\n",
    "        \n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "        self.punctuation_list = [c for c in punctuation]\n",
    "        \n",
    "        # TODO: make this a set\n",
    "        self.STOPWORDS = stopwords.words()\n",
    "\n",
    "        # TODO: Get more complete list and read it from file\n",
    "        MORESTOP = ['will', 'thing', 'n\\'t', '\\'\\'', '\\'s', '``', '\\'re', '\\'', 'mr', 'mr.', '--', '...', '..', '->', '\\'.',\n",
    "                    '\\' \\'', ' .', '’',\n",
    "                    '“', '”', \"\", \"\\n\"]\n",
    "        self.STOPWORDS.extend(MORESTOP)\n",
    "\n",
    "    def substitute_contractions(self, words):\n",
    "        \"\"\"\n",
    "        Loop through words and sub contractions\n",
    "        :param text:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        subbed = []\n",
    "        for word in words:\n",
    "            subbed.append(substitute_contraction(word))\n",
    "        return subbed\n",
    "\n",
    "    def get_non_stopwords(self, words, substitute_contractions=True, stem=True):\n",
    "        \"\"\"\n",
    "        Returns a list of lowercase non-stopwords in the text.\n",
    "        non-stopwords are anything that is not punctuation or stopwords\n",
    "        Numerical values are NOT FILTERED OUT\n",
    "        :param text:\n",
    "        :param stem:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if substitute_contractions:\n",
    "            words = self.substitute_contractions(words)\n",
    "\n",
    "        non_stop_words = []\n",
    "\n",
    "        # Loop through tokens\n",
    "        for word in words:\n",
    "            # Slowing things down\n",
    "            token = self.remove_punctuation(word.lower())\n",
    "            if token not in self.STOPWORDS:\n",
    "                # Check if token contains punctuation\n",
    "                if token not in self.punctuation_list:\n",
    "                    if stem:\n",
    "                        non_stop_words.append(self.get_word_lemma(token))\n",
    "                    else:\n",
    "                        non_stop_words.append(token)\n",
    "\n",
    "        return non_stop_words\n",
    "    \n",
    "\n",
    "    def get_word_lemma(self, word):\n",
    "        \"\"\"\n",
    "        Helper to allows customization to stemming process, like checking for trailing e's\n",
    "        :param word:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        lema = self.lemmatizer.lemmatize(word)\n",
    "        return lema\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        \"\"\"\n",
    "        Helper function to remove all non-acsii charcters\n",
    "        :param text:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "        return ''.join([i if ord(i) < 128 else '' for i in text])\n",
    "\n",
    "    def is_text_token(self, token):\n",
    "        \"\"\"\n",
    "        Checks if not punc or numerical, or non-acsii\n",
    "        :param token:\n",
    "        :return:\n",
    "        \"\"\"\n",
    "\n",
    "        if len(token) == 1:\n",
    "            if ord(token) < 128 and token not in punctuation and not token.isdigit():\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "\n",
    "        else:\n",
    "            if not token.isdigit():\n",
    "                return True\n",
    "            else:\n",
    "                return False\n",
    "    \n",
    "# Perform each type of preprocesing\n",
    "# TODO: Add \"sum+lem+stop+con\"\n",
    "corpora = [\"lem+stop+con\", \"stop+con\", \"stop\", \"none\"]\n",
    "processed_texts = {cor:[] for cor in corpora}\n",
    "processor = LanguageProcessor()\n",
    "\n",
    "\n",
    "# # Reload processed text\n",
    "# import pickle\n",
    "# # Loading files from pickle\n",
    "# processed_texts = pickle.load(open(\"processed_text.pkl\", \"rb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the documents for each type of preprocessing.\n",
    "- lem = lematization\n",
    "- stop = stopword removal\n",
    "- con = expanded contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Pre-processing Text...\")\n",
    "\n",
    "print(\"Processing lem+stop+con\")\n",
    "for doc in alldocs:\n",
    "    words = processor.get_non_stopwords(doc.words, substitute_contractions=True, stem=True)\n",
    "    doc2 = SentimentDocument(words, doc.tags, doc.split, doc.sentiment)\n",
    "    processed_texts[\"lem+stop+con\"].append(doc2)\n",
    "    \n",
    "print(\"Processing stop+con\")\n",
    "for doc in alldocs:\n",
    "    words = processor.get_non_stopwords(doc.words, substitute_contractions=True, stem=False)\n",
    "    doc2 = SentimentDocument(words, doc.tags, doc.split, doc.sentiment)\n",
    "    processed_texts[\"stop+con\"].append(doc2)\n",
    "    \n",
    "print(\"Processing stop\")\n",
    "for doc in alldocs:\n",
    "    words = processor.get_non_stopwords(doc.words, substitute_contractions=False, stem=False)\n",
    "    doc2 = SentimentDocument(words, doc.tags, doc.split, doc.sentiment)\n",
    "    processed_texts[\"stop\"].append(doc2)\n",
    "\n",
    "processed_texts[\"none\"] = alldocs\n",
    "\n",
    "print(\"Completed Pre-processing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setup for evaluating models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "from gensim.test.test_doc2vec import ConcatenatedDoc2Vec\n",
    "import multiprocessing\n",
    "\n",
    "# Keep track of the error rates for each model\n",
    "error_rates = {}\n",
    "\n",
    "def logistic_regression_predictor(X, y):\n",
    "    \"\"\"\n",
    "    Return the predictor after fitting a model on embeddings and sentiment class\n",
    "    :param X: \n",
    "        Embeddings\n",
    "    :param y: \n",
    "        Sentiment class\n",
    "    :return: \n",
    "    \"\"\"\n",
    "    clf = LogisticRegression(random_state=0, verbose=True).fit(X, y)\n",
    "    return clf\n",
    "\n",
    "\n",
    "def model_error_rate(doc2vec_model, train, test):\n",
    "    \"\"\"\n",
    "    Test error rate of regression model that uses the doc2vec embeddings to predict sentiment class\n",
    "    :param doc2vec_model: \n",
    "    :param train: \n",
    "    :param test: \n",
    "    :return: \n",
    "    \"\"\"\n",
    "    \n",
    "    train_y = [doc.sentiment for doc in train_docs]\n",
    "    train_x = [doc2vec_model.docvecs[doc.tags[0]] for doc in train_docs]\n",
    "    test_x = [doc2vec_model.docvecs[doc.tags[0]] for doc in test_docs]\n",
    "    test_y = [doc.sentiment for doc in test]\n",
    "\n",
    "    print(\"Sample Data\", train_x[:1], train_y[:1])\n",
    "    print(f\"\"\"Train / test data breakdown:\n",
    "           Train positive sentiment samples {train_y.count(1.0)} out of {len(train_y)}\n",
    "           Test positive sentiment samples {test_y.count(1.0)} out of {len(test_y)}\"\"\")\n",
    "    \n",
    "    predictor = logistic_regression_predictor(train_x, train_y)\n",
    "    test_predictions = predictor.predict(test_x)\n",
    "\n",
    "    corrects = sum(np.rint(test_predictions) == test_y)\n",
    "    errors = len(test_predictions) - corrects\n",
    "    error_rate = float(errors) / len(test_predictions)\n",
    "    \n",
    "    return error_rate, errors, len(test_predictions), predictor\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# Common Doc2vec configuration\n",
    "common_kwargs = dict(\n",
    "    vector_size=100, epochs=20, min_count=2,\n",
    "    sample=0, workers=multiprocessing.cpu_count(), negative=5, hs=0,\n",
    ")\n",
    "\n",
    "\n",
    "# Doc2vec models for each type of preproccesing\n",
    "models_by_corpora = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Train and evalute a PV-DBOW (paragraph vector distributed bag of words) model for each of the types of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec models ...\n",
      "--------------------\n",
      "Model trained on: lem+stop+con\n",
      "Sample Data [array([ 0.19048014,  0.18574472,  0.19675452,  0.34363356, -0.05429327,\n",
      "       -0.34967002, -0.9629098 ,  0.5046224 ,  0.3553929 , -0.02000841,\n",
      "        0.42241308,  0.31711754, -0.1987363 , -0.11315008, -0.06220058,\n",
      "       -0.0768768 ,  0.22785848,  0.2949146 , -0.39035028,  0.26064464,\n",
      "       -0.11486331,  0.13627096,  0.38031614,  0.03657763,  0.4365639 ,\n",
      "       -0.5859031 , -0.7461953 ,  0.551983  , -0.7311374 ,  0.46559998,\n",
      "       -0.38593325, -0.44615978, -0.30371758, -0.05886084,  0.11023851,\n",
      "       -0.15133354,  0.37857378, -0.35254526, -0.05427878, -0.0111264 ,\n",
      "       -0.5489169 ,  0.41485846,  0.50775194,  0.36012176, -0.171678  ,\n",
      "        0.24442673, -0.64688766, -0.28453165, -0.38347286,  0.4489134 ,\n",
      "       -0.25286022,  0.00631496,  0.04119195, -0.15534724,  0.68830603,\n",
      "       -0.23483275,  0.14449677, -0.4099465 ,  0.12440899,  0.09973439,\n",
      "       -0.0247636 ,  0.02926677, -0.54103404,  0.4465653 , -0.11672283,\n",
      "        0.64713603, -0.64009285, -0.18964896,  0.1096048 , -0.38112378,\n",
      "        0.26582056,  0.0032023 , -0.12880178,  0.65239453,  0.16594951,\n",
      "       -0.16200961, -0.60751504, -0.16466144,  0.23449329,  0.22097771,\n",
      "        0.4549512 , -0.19687998, -0.09566788, -0.24440518,  0.73698527,\n",
      "        0.81102914, -0.0922519 ,  0.53137237,  0.67719966, -0.64504015,\n",
      "        0.19332719,  0.03491812, -0.28868482,  0.43951693, -0.5848366 ,\n",
      "        0.28153417,  0.11109039,  0.36863866,  0.01903613, -0.09364004],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 0 out of 25000\n",
      "           Test positive sentiment samples 0 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.10584 for model Doc2Vec(dbow,d100,n5,mc2,t4) trained on lem+stop+con\n",
      "--------------------\n",
      "--------------------\n",
      "Model trained on: stop+con\n",
      "Sample Data [array([-0.37394282,  0.14541703, -0.1181206 ,  0.32588914,  0.00417809,\n",
      "       -0.40655154, -0.8618801 , -0.17082322,  0.35712767, -0.34439042,\n",
      "        0.36993855,  0.5819138 , -0.54412836,  0.24266304,  0.2662939 ,\n",
      "        0.0190639 ,  0.83017415,  0.12247381, -0.5949772 ,  0.12477303,\n",
      "        0.09859015,  0.004922  ,  0.3735908 ,  0.01978863, -0.05533889,\n",
      "       -0.4187592 , -0.5312697 ,  0.42291212, -0.19113207,  0.11957824,\n",
      "       -0.2275183 , -0.17485602, -0.01526071, -0.14402497,  0.3751672 ,\n",
      "       -0.10791633,  0.03592143, -0.5977169 , -0.23647137, -0.16427007,\n",
      "       -0.25056264,  0.31526288,  0.38313723,  0.32543314,  0.15905464,\n",
      "        0.22170836, -0.15616153, -0.2102895 , -0.4667185 ,  0.3467459 ,\n",
      "       -0.33510837, -0.29897165,  0.3037702 , -0.7112232 ,  0.32905924,\n",
      "        0.662519  ,  0.17032981, -0.11783642,  0.25756025,  0.23571575,\n",
      "       -0.51707953,  0.24368866, -0.17814505,  0.32024974,  0.38904104,\n",
      "        0.36941594, -1.1284411 , -0.23267248, -0.33957085, -0.48451012,\n",
      "       -0.13348551,  0.3809017 ,  0.52697206,  0.33489567,  0.31640992,\n",
      "        0.08214362, -0.6912635 , -0.1326139 ,  0.5050591 ,  0.19532432,\n",
      "        0.45695338, -0.14653847,  0.23882523, -0.00942939,  0.26533264,\n",
      "        0.92106867,  0.12358935,  0.03762313,  0.6187305 , -0.2032513 ,\n",
      "        0.09500759, -0.28814957, -0.3595377 , -0.00564421, -0.3777478 ,\n",
      "        0.32853693, -0.01372158,  0.09902897, -0.7397639 ,  0.46761686],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 0 out of 25000\n",
      "           Test positive sentiment samples 0 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.3s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.10372 for model Doc2Vec(dbow,d100,n5,mc2,t4) trained on stop+con\n",
      "--------------------\n",
      "--------------------\n",
      "Model trained on: stop\n",
      "Sample Data [array([ 0.17761402, -0.26420492,  0.361937  ,  0.26289362, -0.4712735 ,\n",
      "       -0.38532922, -0.51815885, -0.04005979,  0.1153435 , -0.2119986 ,\n",
      "        0.25628793,  0.35129106, -0.27893662, -0.20348997,  0.6807101 ,\n",
      "       -0.01895518,  0.8363311 ,  0.15891616, -0.5509589 , -0.30133876,\n",
      "        0.1803388 ,  0.19554168,  0.2696354 , -0.12448578,  0.32309046,\n",
      "       -0.32536864, -0.23913145,  0.44343346, -0.2880625 ,  0.37692592,\n",
      "        0.18396729,  0.04414753, -0.27661565,  0.590954  ,  0.4049222 ,\n",
      "        0.22829579, -0.06214761, -0.34437808,  0.16237988, -0.03092983,\n",
      "       -0.25630778,  0.39341688,  0.09050135, -0.02627868, -0.34114236,\n",
      "        0.27708352, -0.38553795, -0.2610582 , -0.25584152,  0.34392452,\n",
      "       -0.53384113,  0.51197445,  0.3515095 , -0.73996824,  0.17246945,\n",
      "        0.68162835,  0.23187435, -0.48883978,  0.21458243,  0.06198528,\n",
      "       -0.11231139,  0.11473294, -0.1376906 ,  0.4069434 ,  0.10178344,\n",
      "        0.05702917, -1.142272  , -0.66187847,  0.19556211, -0.5758289 ,\n",
      "        0.3369154 ,  0.47082347,  0.28690955,  0.15663663,  0.5783916 ,\n",
      "        0.09882393, -0.46439365, -0.07500058, -0.08380714,  0.19323291,\n",
      "        0.63889927, -0.17160232, -0.18992092, -0.06039828,  0.5978504 ,\n",
      "        1.101122  , -0.23569529, -0.06783999,  0.44590482, -0.6859156 ,\n",
      "        0.4884825 ,  0.07966238, -0.26246306,  0.03307385, -0.10990991,\n",
      "        0.2760217 ,  0.04292908,  0.3876102 , -0.45853728,  0.00556632],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 0 out of 25000\n",
      "           Test positive sentiment samples 0 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.10392 for model Doc2Vec(dbow,d100,n5,mc2,t4) trained on stop\n",
      "--------------------\n",
      "--------------------\n",
      "Model trained on: none\n",
      "Sample Data [array([-3.38244528e-01, -2.18791798e-01, -2.18285546e-01,  4.88449216e-01,\n",
      "        2.68145382e-01, -2.59598464e-01, -1.01595536e-01, -4.23042983e-01,\n",
      "       -3.71159554e-01, -2.55106032e-01,  2.21404284e-01, -2.24932402e-01,\n",
      "       -1.24065077e+00, -5.14121614e-02, -2.08528668e-01, -9.39034671e-02,\n",
      "        1.84628502e-01, -4.37048882e-01, -1.55302763e-01, -7.63706982e-01,\n",
      "       -1.42467797e-01, -3.85564417e-01, -9.14833415e-03, -5.12101129e-02,\n",
      "       -1.68125749e-01, -1.40146643e-01, -5.10644257e-01, -1.15374245e-01,\n",
      "       -3.22570175e-01,  6.44028544e-01,  3.59059125e-01,  3.49316210e-01,\n",
      "        3.70865494e-01, -2.23197460e-01,  1.43083604e-02,  1.52120396e-01,\n",
      "        1.07828908e-01, -2.10189559e-02, -1.34678394e-01, -4.68377799e-01,\n",
      "       -1.36552170e-01,  9.28948283e-01, -3.62675935e-02,  9.40234840e-01,\n",
      "        7.50751793e-01,  1.03320301e+00,  2.81464100e-01, -6.86070919e-02,\n",
      "       -2.09628329e-01,  2.43866980e-01,  2.73190856e-01,  1.01606928e-01,\n",
      "       -1.60753936e-01, -8.26742887e-01, -1.83281064e-01,  1.52872205e-01,\n",
      "        1.34328324e-02, -4.87752050e-01,  1.19314931e-01, -1.15016788e-01,\n",
      "       -1.46819651e-01,  9.03274044e-02, -9.21091214e-02, -2.08205447e-01,\n",
      "        8.24902117e-01, -1.64929062e-01, -9.80543554e-01,  2.54118025e-01,\n",
      "        4.80116487e-01, -7.08512843e-01,  1.70284092e-01,  3.88119042e-01,\n",
      "        3.87340456e-01, -8.09154361e-02, -1.04900733e-01, -4.82449512e-04,\n",
      "       -5.52881956e-01,  7.14688957e-01,  6.92867279e-01,  1.72181219e-01,\n",
      "        7.73343220e-02, -1.55772522e-01, -1.51177853e-01,  4.02067065e-01,\n",
      "        3.27824354e-01,  4.45507556e-01, -1.78251252e-01, -4.14936334e-01,\n",
      "        1.24737740e-01, -4.80593681e-01,  4.07060057e-01, -6.14810526e-01,\n",
      "       -9.06153977e-01, -5.93805313e-01, -2.76148766e-02,  5.27892768e-01,\n",
      "       -5.79665303e-01,  3.18450183e-01, -3.32328975e-01, -4.09385800e-01],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 0 out of 25000\n",
      "           Test positive sentiment samples 0 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.10152 for model Doc2Vec(dbow,d100,n5,mc2,t4) trained on none\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "for model in corpora:\n",
    "    \n",
    "    # TODO: Concatenated doc2vec model perform slightly better, however it is missing\n",
    "    # tags property in it's implementation.\n",
    "    # 2nd best model used\n",
    "    models_by_corpora[model] = Doc2Vec(dm=0, **common_kwargs)\n",
    "    \n",
    "    \n",
    "print(\"Training Doc2Vec models ...\")\n",
    "\n",
    "# Evaluate Doc2vec for each type of preproccesing\n",
    "for corpus in corpora:\n",
    "    \n",
    "    model = models_by_corpora[corpus]\n",
    "    docs = processed_texts[corpus]\n",
    "    \n",
    "    print(\"-\"*20)\n",
    "    print(f\"Training model on: {corpus} documents ...\")\n",
    "    \n",
    "    # Split into train / test sets\n",
    "    train_docs = [doc for doc in docs if doc.split == 'train']\n",
    "    test_docs = [doc for doc in docs if doc.split == 'test']\n",
    "    \n",
    "    model.build_vocab(docs)\n",
    "    model.train(docs, total_examples=len(docs), epochs=model.epochs)\n",
    "\n",
    "    err_rate, err_count, test_count, predictor = model_error_rate(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    \n",
    "    print(f\"Error rate: {err_rate} for model {str(model)} trained on {corpus}\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the PV-DBOW model no performing any preproccesing produces the highest accuracy.\n",
    "\n",
    "Train and evalute a PV-DM (paragraph vector distributed memory) model for each of the types of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Doc2Vec models ...\n",
      "--------------------\n",
      "Training model on: lem+stop+con documents ...\n",
      "Sample Data [array([ 4.8964587e-01, -5.8797348e-02, -2.6077956e-01,  4.5133162e-01,\n",
      "       -1.5007243e-01,  3.4983036e-01, -1.4829528e-01,  1.1516626e+00,\n",
      "       -1.4036475e-01, -4.4391423e-01,  2.6714194e-01,  2.4020796e-01,\n",
      "        1.8720028e-01, -3.7481549e-01,  1.1214490e+00,  4.3689597e-01,\n",
      "       -6.3363835e-02, -9.4921246e-02, -2.2934680e-01, -3.0742434e-01,\n",
      "        1.7611554e-01, -1.1198944e-03, -1.1979154e-01, -1.4307345e-01,\n",
      "        3.8967717e-01,  7.8051817e-01,  4.0083644e-01,  3.1800258e-01,\n",
      "        7.9142708e-01, -1.3788615e-01, -6.3872203e-02, -1.8768595e-01,\n",
      "       -3.5029519e-01, -3.0670562e-01, -4.6106091e-01,  5.6179827e-01,\n",
      "        2.3946853e-01,  4.8041072e-01, -4.1785136e-01,  2.2026943e-01,\n",
      "        9.6610361e-01, -4.8661163e-01, -2.3579098e-01, -7.3455429e-01,\n",
      "        1.0253838e+00,  5.8691138e-01,  6.9736248e-01, -9.4024286e-02,\n",
      "       -6.7753464e-01,  8.4717132e-02, -3.2288906e-01, -7.3024648e-01,\n",
      "        3.4105018e-01,  1.7702815e-01,  3.6370891e-01, -3.6651492e-01,\n",
      "        6.7028689e-01, -6.2619513e-01,  6.6040194e-01, -2.7155226e-01,\n",
      "       -4.5966882e-01, -2.7968913e-01,  7.3509771e-01,  1.1075566e+00,\n",
      "       -1.0244410e+00, -8.6179145e-02,  3.3973897e-01, -4.9158314e-01,\n",
      "       -1.1459063e+00, -8.8120483e-02,  3.0649221e-01,  2.3029993e-01,\n",
      "        6.6662967e-01, -1.8320113e-02, -5.7732028e-01,  8.4852487e-01,\n",
      "       -2.4082957e-01,  1.8521760e-01, -1.3982904e-01,  4.1279918e-01,\n",
      "        8.7412250e-01, -1.5827331e-01, -4.5995748e-01, -1.0220734e+00,\n",
      "       -5.9550828e-01, -5.1611763e-01, -1.5298973e-01, -1.1561557e+00,\n",
      "       -2.3052068e-01,  3.2421118e-01,  8.4572953e-01,  8.6495228e-02,\n",
      "        1.7998239e-01, -5.6005919e-01,  3.4209199e-02, -3.1384557e-01,\n",
      "        6.2168914e-01, -3.9444017e-01,  1.3564380e-01,  6.4791024e-01],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 12500 out of 25000\n",
      "           Test positive sentiment samples 12500 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.2s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.14544 for model Doc2Vec(dm/m,d100,n5,w5,mc2,t4) trained on lem+stop+con\n",
      "--------------------\n",
      "Training model on: stop+con documents ...\n",
      "Sample Data [array([ 0.27232426, -0.45932883,  0.5505049 , -0.1618307 , -0.65958476,\n",
      "        0.8766121 , -0.2310929 ,  0.9546105 , -0.1475887 , -0.7298239 ,\n",
      "        0.9234847 ,  0.24537049, -0.08019847, -1.2477479 ,  0.9693112 ,\n",
      "        0.31118992, -0.48314756, -0.16000451, -0.56573766,  0.04264857,\n",
      "        0.5580025 , -0.25024548, -0.9058656 , -0.21374385,  0.30061185,\n",
      "        0.78815556, -0.35189217,  0.05987649,  0.9073889 ,  0.01899325,\n",
      "        0.6163013 , -0.38901085,  0.17431754,  0.4312105 ,  0.22678795,\n",
      "        0.16277197,  0.4662049 ,  0.20152847,  0.0473839 , -0.41274124,\n",
      "        0.10997185, -0.4706153 ,  0.01948678,  0.7231294 ,  0.67650867,\n",
      "       -0.12370142,  0.27910763, -0.29406056, -0.18828371,  0.6250889 ,\n",
      "       -0.20583498, -0.3819209 , -0.13838315, -0.29362363, -0.450509  ,\n",
      "       -0.81792045,  0.81960404, -0.11443598,  0.39497218, -0.05797862,\n",
      "       -0.05841164, -0.18814534,  0.9809354 ,  0.7236521 , -0.60241777,\n",
      "        0.5301372 ,  0.38385022, -0.33974707, -0.9334583 , -0.13478659,\n",
      "       -0.01314063,  0.2965984 ,  0.3774262 ,  0.25089425, -0.4181152 ,\n",
      "        0.61065096, -0.39635828,  0.50315773,  0.4449255 ,  0.21391636,\n",
      "        0.687516  ,  0.12915942, -0.40371534, -0.30597648, -0.6364073 ,\n",
      "       -0.28907093, -0.3940327 , -1.3141736 ,  0.285172  ,  0.41433144,\n",
      "       -0.60395116,  0.34846047, -0.04287337, -0.1988266 ,  0.12100979,\n",
      "       -0.7020437 ,  0.80700165, -0.02938765, -0.8093198 ,  0.7709637 ],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 12500 out of 25000\n",
      "           Test positive sentiment samples 12500 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.14572 for model Doc2Vec(dm/m,d100,n5,w5,mc2,t4) trained on stop+con\n",
      "--------------------\n",
      "Training model on: stop documents ...\n",
      "Sample Data [array([ 3.1782696e-01,  1.4468937e-01,  5.5200112e-01,  1.9754238e-01,\n",
      "       -4.5716739e-01, -2.0876018e-02,  5.3391922e-02,  4.2869037e-01,\n",
      "        3.5506062e-02, -5.7764381e-01,  2.9031059e-01,  9.8971464e-02,\n",
      "       -1.8032686e-01, -8.9375979e-01,  6.4667416e-01, -3.0933955e-01,\n",
      "       -3.0709907e-01, -8.9960583e-02, -2.2077662e-01,  4.5606655e-01,\n",
      "        1.0671389e+00,  7.4585646e-02, -4.4430754e-01, -1.5670855e-01,\n",
      "        7.2458327e-01,  5.3552294e-01, -6.2834823e-01,  3.3240920e-01,\n",
      "        3.6941335e-01,  6.1231934e-02,  7.6540363e-01, -4.9100927e-01,\n",
      "        3.1980252e-01,  9.9475071e-02, -3.2293942e-02, -4.1922846e-01,\n",
      "        6.7690104e-01,  5.3153348e-01,  4.3957984e-01, -1.9703147e-01,\n",
      "        6.0100913e-01, -6.1769575e-01,  1.7554343e-01,  5.2654654e-01,\n",
      "        3.0081433e-01,  2.2253822e-01,  1.8421561e-01,  4.0907881e-04,\n",
      "        1.2623391e-01, -6.7707136e-02, -2.8881404e-01, -2.3946987e-01,\n",
      "        3.3792603e-01, -1.0322932e+00,  2.4921082e-01, -8.6286998e-01,\n",
      "        6.2648803e-01, -5.5888301e-01,  9.4662711e-02,  2.5411090e-01,\n",
      "       -8.8286227e-01, -6.2045830e-01,  9.2910999e-01,  6.8009233e-01,\n",
      "       -1.5314434e-01,  4.0674376e-01,  4.5732135e-01, -4.4963634e-01,\n",
      "       -9.2887509e-01,  5.4273289e-01,  5.6189841e-01,  2.0226634e-01,\n",
      "        2.0317608e-01,  4.4167739e-01, -5.8823967e-01,  1.0067676e+00,\n",
      "       -3.8575375e-01,  5.3689986e-01, -2.3127054e-01,  3.3599576e-01,\n",
      "        7.0743680e-01,  4.1807556e-01, -5.5639392e-01, -2.5930837e-01,\n",
      "       -5.2398920e-01, -3.2202455e-01,  1.5722957e-01, -1.7345572e+00,\n",
      "        8.3382320e-01,  4.0128851e-01, -1.5007350e-01, -1.6074151e-01,\n",
      "        1.6447368e-01, -2.3829909e-01,  2.5219926e-01, -9.1722220e-01,\n",
      "        1.1945024e+00, -3.3922754e-03, -4.6246690e-01, -6.1687022e-02],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 12500 out of 25000\n",
      "           Test positive sentiment samples 12500 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.9s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.14364 for model Doc2Vec(dm/m,d100,n5,w5,mc2,t4) trained on stop\n",
      "--------------------\n",
      "Training model on: none documents ...\n",
      "Sample Data [array([-0.4539063 ,  0.41374287, -0.45566967, -0.1684033 ,  0.25140274,\n",
      "        1.457889  , -0.8647763 , -1.1876287 ,  1.2879443 , -0.7200864 ,\n",
      "       -0.6822012 ,  0.48079208, -0.19480446, -0.5912165 , -0.13309531,\n",
      "        0.29199585,  0.33319795, -1.7095803 ,  0.12073743, -0.13520083,\n",
      "        0.9674745 ,  0.70850074,  0.63916624, -0.29646635,  0.4273372 ,\n",
      "        0.0195256 , -0.6486902 ,  0.790284  ,  0.37703988,  0.5091603 ,\n",
      "        0.29243577, -1.3375407 ,  0.3686426 ,  0.25683177, -0.04735296,\n",
      "       -0.02597161,  0.6014649 ,  0.69863445, -0.3714222 ,  1.5299557 ,\n",
      "       -1.2560995 , -0.66209966, -0.11763109,  0.9145844 ,  0.74525046,\n",
      "        0.1489802 , -0.34050682,  0.8036674 ,  0.938801  ,  0.31990236,\n",
      "       -0.08393154, -0.6542425 ,  0.29231068,  0.9085804 ,  0.2648597 ,\n",
      "        0.83380234, -0.18929778,  0.559436  , -0.4640625 ,  0.5349132 ,\n",
      "        0.7523026 , -0.01302132,  1.089498  ,  0.44429344, -0.4427857 ,\n",
      "        1.4064068 , -0.2125065 , -0.99671614,  0.57929593,  0.38906014,\n",
      "        0.37499443, -0.338992  , -0.26172623,  1.4000531 , -0.54656   ,\n",
      "        0.41310653,  0.4035255 , -0.42869154,  1.1621591 , -0.32267082,\n",
      "        0.02732356, -1.1190249 , -0.31117377,  0.42299557, -1.3349978 ,\n",
      "       -0.26990575,  0.3766721 , -1.5723923 , -0.74417454, -0.22910331,\n",
      "       -0.9478052 , -0.3581166 ,  0.34648785,  0.33728606,  0.43224514,\n",
      "       -0.39758807, -1.6225557 , -1.4065316 ,  0.22539225, -0.69809586],\n",
      "      dtype=float32)] [1.0]\n",
      "Train / test data breakdown:\n",
      "           Train positive sentiment samples 12500 out of 25000\n",
      "           Test positive sentiment samples 12500 out of 25000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error rate: 0.17256 for model Doc2Vec(dm/m,d100,n5,w5,mc2,t4) trained on none\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.5s finished\n"
     ]
    }
   ],
   "source": [
    "for model in corpora:\n",
    "    \n",
    "    # TODO: Concatenated doc2vec model perform slightly better, however it is missing\n",
    "    # tags property in it's implementation.\n",
    "    # 2nd best model used\n",
    "    models_by_corpora[model + \"+dm\"] = Doc2Vec(dm=1, **common_kwargs)\n",
    "    \n",
    "    \n",
    "print(\"Training Doc2Vec models ...\")\n",
    "\n",
    "# Evaluate Doc2vec for each type of preproccesing\n",
    "for corpus in corpora:\n",
    "    \n",
    "    model = models_by_corpora[corpus + \"+dm\"]\n",
    "    docs = processed_texts[corpus]\n",
    "    \n",
    "    print(\"-\"*20)\n",
    "    print(f\"Training model on: {corpus} documents ...\")\n",
    "    \n",
    "    # Split into train / test sets\n",
    "    train_docs = [doc for doc in docs if doc.split == 'train']\n",
    "    test_docs = [doc for doc in docs if doc.split == 'test']\n",
    "\n",
    "    model.build_vocab(docs)\n",
    "    model.train(docs, total_examples=len(docs), epochs=model.epochs)\n",
    "\n",
    "    err_rate, err_count, test_count, predictor = model_error_rate(model, train_docs, test_docs)\n",
    "    error_rates[str(model)] = err_rate\n",
    "    \n",
    "    print(f\"Error rate: {err_rate} for model {str(model)} trained on {corpus}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the distrubted memory model the worst performance occurs when no preprocessing is applied (Error rate: 0.17256). The best performance is from removing stopwords (Error rate: 0.14364). Additional preprocessing such as, lem+stop+con and stop+con performs similary. (Error rate: 0.14544 and 0.14572)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "Doc2Vec Training.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
